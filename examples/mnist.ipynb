{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2c528-c090-43f7-8044-e2b59a0ebcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray, PyTree\n",
    "from flax import nnx\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = 'Times'\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('../')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f920dc-60fa-4dea-b35d-cd191b28b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 10\n",
    "TASK = [0, 1]\n",
    "LAYER_SIZES = [784, 100, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996b186-045b-43c1-80d1-bbaf68f17787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Lambda(lambda x: jnp.array(x.numpy())/255.0),\n",
    "])\n",
    "train = datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test = datasets.MNIST(\n",
    "    root=\"data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "def collate_fn(batch: list[tuple[jax.Array, int]]) -> tuple[jax.Array, jax.Array]:\n",
    "    inputs = [s[0] for s in batch]\n",
    "    targets = [s[1] for s in batch]\n",
    "    input_batch = jnp.stack(inputs, axis=0)\n",
    "    target_batch = jnp.array(targets)\n",
    "    return input_batch, target_batch\n",
    "    \n",
    "train = Subset(train, [i for i in range(len(train)) if train.targets[i] in TASK])\n",
    "test_id = Subset(test, [i for i in range(len(test)) if test.targets[i] in TASK])\n",
    "test_ood = Subset(test, [i for i in range(len(test)) if test.targets[i] not in TASK])\n",
    "NUM_DATA_POINTS = len(train)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_id = DataLoader(test_id, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader_ood = DataLoader(test_ood, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c7e85-edbd-41fd-9444-8634c83f4695",
   "metadata": {},
   "source": [
    "## Find MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd9d53-af1b-4d39-a563-74f87c7e49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometric_bayesian.models import MLP\n",
    "model = MLP(\n",
    "    layers=LAYER_SIZES,\n",
    "    prob=False,\n",
    "    param_dtype=jax.numpy.float64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced6a4e-54c9-493d-b8b2-ae06b084075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometric_bayesian.densities import Bernoulli, MultivariateNormal\n",
    "from geometric_bayesian.functions.likelihood import neg_logll\n",
    "from geometric_bayesian.operators import DiagOperator\n",
    "from geometric_bayesian.utils.helper import pytree_to_array, array_to_pytree\n",
    "\n",
    "p_ll = lambda f : Bernoulli(f, logits=True)\n",
    "\n",
    "num_params = sum(p.size for p in jax.tree_util.tree_leaves(nnx.state(model)))\n",
    "prior_var = DiagOperator(jnp.array(100.), num_params)\n",
    "p_prior = MultivariateNormal(prior_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d8d09b-8428-4599-a264-ccc756f8cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(LEARNING_RATE))\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred = model(x)\n",
    "    return neg_logll(p_ll, y, y_pred) #- p_prior(pytree_to_array(nnx.state(model)))/y.shape[0]\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, y):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, x, y)\n",
    "    optimizer.update(grads) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1c391-7164-434a-8872-d3d89d12e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for x_tr, y_tr in pbar:\n",
    "        losses.append(train_step(model, optimizer, x_tr.reshape(x_tr.shape[0], -1), y_tr).item())\n",
    "        epoch_losses.append(losses[-1])\n",
    "        avg_loss = sum(epoch_losses)/len(epoch_losses)\n",
    "        pbar.set_postfix({\n",
    "                'loss': f'{float(losses[-1]):.4f}',\n",
    "                'avg_loss': f'{avg_loss:.4f}'\n",
    "            })\n",
    "print(f'{optimizer.step.value = }')\n",
    "print(f\"Final loss: {losses[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efaa92d-a1c9-41f8-a361-586e757fc50c",
   "metadata": {},
   "source": [
    "## Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e09337-2c29-414e-8c21-39e74bef8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometric_bayesian.curv.ggn import ggn\n",
    "from geometric_bayesian.utils.helper import wrap_pytree_function, pytree_to_array, array_to_pytree\n",
    "from geometric_bayesian.operators import PSDOperator\n",
    "\n",
    "mini_batch: dict[str, Array] = next(iter(train_loader))\n",
    "graph_def, map_params = nnx.split(model)\n",
    "\n",
    "ggn_fn = wrap_pytree_function(\n",
    "    ggn(\n",
    "        p = p_ll,\n",
    "        f = model,\n",
    "        X = mini_batch[0].reshape(mini_batch[0].shape[0],-1),\n",
    "        y = mini_batch[1],\n",
    "    ), \n",
    "    map_params\n",
    ")\n",
    "\n",
    "ggn_mv = lambda v : ggn_fn(pytree_to_array(map_params), v)\n",
    "ggn_op = PSDOperator(ggn_mv, op_size=num_params)\n",
    "ggn_lr = ggn_op.lowrank(num_modes=150, method='lobpcg')\n",
    "cov_op = (ggn_lr + p_prior._cov).inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad4279-a152-45e6-b94f-217fda7e6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.api import GGN\n",
    "from laplax.util.flatten import full_flatten\n",
    "import numpy as np\n",
    "\n",
    "graph_def, map_params = nnx.split(model)\n",
    "def model_fn(input,params):\n",
    "    return nnx.call((graph_def, params))(input.reshape(input.shape[0],-1))[0]\n",
    "\n",
    "SCALING_FACTOR = 1 / BATCH_SIZE\n",
    "\n",
    "ggn_mv = GGN(\n",
    "    model_fn,\n",
    "    map_params,\n",
    "    data = mini_batch,\n",
    "    loss_fn = \"binary_cross_entropy\",\n",
    "    vmap_over_data = True,\n",
    "    factor = SCALING_FACTOR,\n",
    ")\n",
    "\n",
    "test_laplax = full_flatten(ggn_mv(map_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8284e79-502f-4d80-ad43-f24d1ac43e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.allclose(test_laplax,ggn_op(pytree_to_array(map_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ec336-eaf7-442f-9cb1-03fce885515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometric_bayesian.densities import MultivariateNormal\n",
    "from geometric_bayesian.approx.mc import pred_posterior_mean, pred_posterior_std, pred_posterior\n",
    "\n",
    "posterior = MultivariateNormal(cov=cov_op, mean=pytree_to_array(map_params))\n",
    "params_samples = posterior.sample(size=100, num_modes=150, method='lobpcg')\n",
    "\n",
    "X_test, y_test = next(iter(test_loader_id))\n",
    "\n",
    "mean_fn = pred_posterior_mean(model, params_samples, num_modes=150, method='lobpcg')\n",
    "std_fn = pred_posterior_std(model, params_samples, num_modes=150, method='lobpcg')\n",
    "pred_posterior_fn = pred_posterior(model, params_samples, p_ll, num_modes=150, method='lobpcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a59a06-b7fe-47b3-ace2-fe7ef283be98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
